{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem.porter import *\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Lab7_TextAnalysisChristmasSongsFull.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = ''\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            result += ' ' + (lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "df['mod_text'] = df['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_df(text_column):\n",
    "    x = text_column.map(analyzer.polarity_scores)\n",
    "    y = [[x[i][j] for i in range(len(x))] for j in ['neg', 'neu', 'pos', 'compound']]\n",
    "    return y\n",
    "df['neg'], df['neu'], df['pos'], df['compound'] = sentiment_df(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "      <th>songID</th>\n",
       "      <th>Popular</th>\n",
       "      <th>mod_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>We Wish You A Merry Christmas</td>\n",
       "      <td>We wish you a merry Christmas  \\r\\r\\nWe wish y...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wish merri christma wish merri christma wish ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Air Supply</td>\n",
       "      <td>Sleigh Ride</td>\n",
       "      <td>Just hear those sleigh bells jingling ring tin...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>hear sleigh bell jingl ring ting tingl come l...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Dixie</td>\n",
       "      <td>By now in New York City, there's snow on the g...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>new york citi snow grind california sunshin f...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Your Arms</td>\n",
       "      <td>All my friends are asking me where I plan to s...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>friend ask plan spend holiday peopl celebr se...</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas Is Love</td>\n",
       "      <td>It's that time of year when the whole world is...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>time year world heart tot heart feel love chr...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.9986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist                           song  \\\n",
       "0        ABBA  We Wish You A Merry Christmas   \n",
       "1  Air Supply                    Sleigh Ride   \n",
       "2     Alabama             Christmas In Dixie   \n",
       "3     Alabama         Christmas In Your Arms   \n",
       "4     Alabama              Christmas Is Love   \n",
       "\n",
       "                                                text  songID  Popular  \\\n",
       "0  We wish you a merry Christmas  \\r\\r\\nWe wish y...       1        0   \n",
       "1  Just hear those sleigh bells jingling ring tin...       2        1   \n",
       "2  By now in New York City, there's snow on the g...       3        0   \n",
       "3  All my friends are asking me where I plan to s...       4        0   \n",
       "4  It's that time of year when the whole world is...       5        0   \n",
       "\n",
       "                                            mod_text    neg    neu    pos  \\\n",
       "0   wish merri christma wish merri christma wish ...  0.000  0.582  0.418   \n",
       "1   hear sleigh bell jingl ring ting tingl come l...  0.036  0.751  0.213   \n",
       "2   new york citi snow grind california sunshin f...  0.013  0.801  0.186   \n",
       "3   friend ask plan spend holiday peopl celebr se...  0.081  0.719  0.200   \n",
       "4   time year world heart tot heart feel love chr...  0.000  0.559  0.441   \n",
       "\n",
       "   compound  \n",
       "0    0.9979  \n",
       "1    0.9956  \n",
       "2    0.9690  \n",
       "3    0.9758  \n",
       "4    0.9986  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['mod_text', 'neg', 'neu', 'pos', 'compound']], df['Popular'], test_size=0.25, random_state=181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "log = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "params = {'tfidf__ngram_range': [(1,1),(1,2)], 'log__C':np.logspace(-3,5,1000)}\n",
    "pipe = Pipeline(steps=[('tfidf',tfidf),('log',log)])\n",
    "cv = GridSearchCV(pipe, params, cv=5, verbose=1, n_jobs=-1)\n",
    "cv.fit(X_train['mod_text'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "lda = LatentDirichletAllocation()\n",
    "log = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "params = {'tfidf__ngram_range': [(1,1),(1,2)], 'lda__n_components':range(5,30), 'log__C':np.logspace(-3,5,1000)}\n",
    "pipe = Pipeline(steps=[('tfidf',tfidf),('lda',lda),('log',log)])\n",
    "cv2 = GridSearchCV(pipe, params, cv=5, verbose=1, n_jobs=-1)\n",
    "cv2.fit(X_train['mod_text'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "params = {'log__C':np.logspace(-3,5,1000)}\n",
    "pipe = Pipeline(steps=[('log',log)])\n",
    "cv3 = GridSearchCV(pipe, params, cv=5, verbose=1, n_jobs=-1)\n",
    "cv3.fit(X_train[['neg', 'neu', 'pos', 'compound']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "(C, ngram_range, min_df, max_df, n_components)\n",
    "for C in np.logspace(-3,5,1000)\n",
    "for ngram_range in [(1,1),(1,2),(1,3)]\n",
    "for min_df in [0.1,0.2]\n",
    "for max_df in [0.8,0.9,1.0]\n",
    "for n_components in range(5,30)\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "'C': None,\n",
    "'ngram_range': None,\n",
    "'min_df': None,\n",
    "'max_df': None,\n",
    "'n_components': None\n",
    "}\n",
    "\n",
    "best_cv_score_21 = 0\n",
    "best_cv_score_std_21 = None\n",
    "best_model_21 = None\n",
    "best_parameters_21 = None\n",
    "param = 0\n",
    "for C, ngram_range, min_df, max_df, n_components in gridsearch_params:\n",
    "    cv_scores = []\n",
    "    param +=1\n",
    "    print('param', param)\n",
    "    fold = 0\n",
    "\n",
    "    params['C'] = C\n",
    "    params['ngram_range'] = ngram_range\n",
    "    params['min_df'] = min_df\n",
    "    params['max_df'] = max_df\n",
    "    params['n_components'] = n_components\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        fold +=1\n",
    "        print('fold', fold)\n",
    "\n",
    "        tfidf = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "        temp = tfidf.fit_transform(X_train['mod_text'].iloc[train_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        feature_names = tfidf.get_feature_names()\n",
    "        X_1_tr = pd.DataFrame(dense, columns=feature_names)\n",
    "        temp = tfidf.transform(X_train['mod_text'].iloc[test_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        X_1_te = pd.DataFrame(dense, columns=feature_names)\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=n_components, n_jobs=-1)\n",
    "        temp = lda.fit_transform(X_1_tr)\n",
    "        feature_names = ['topic {}'.format(i) for i in range(1, n_components +1)]\n",
    "        X_2_tr = pd.DataFrame(temp, columns=feature_names)\n",
    "        temp = lda.transform(X_1_te)\n",
    "        X_2_te = pd.DataFrame(temp, columns=feature_names)\n",
    "\n",
    "        \n",
    "        X_cv_train = pd.concat([X_1_tr, X_2_tr], axis=1)\n",
    "        X_cv_test = pd.concat([X_1_te, X_2_te], axis=1)\n",
    "        y_cv_train = df['Popular'].iloc[train_index]\n",
    "        y_cv_test = df['Popular'].iloc[test_index]\n",
    "\n",
    "        log = LogisticRegression(C=C, solver='lbfgs', n_jobs=-1)\n",
    "        log.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "        cv_scores.append(log.score(X_cv_test, y_cv_test))\n",
    "\n",
    "    if np.mean(cv_scores) > best_cv_score_21:\n",
    "        best_cv_score_21 = np.mean(cv_scores)\n",
    "        best_cv_score_std_21 = np.std(cv_scores)\n",
    "        best_parameters_21 = params\n",
    "        best_model_21 = [tfidf, lda, log]\n",
    "\n",
    "    print('score', np.mean(cv_scores))\n",
    "    print('params', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "(C, ngram_range, min_df, max_df)\n",
    "for C in np.logspace(-3,5,1000)\n",
    "for ngram_range in [(1,1),(1,2),(1,3)]\n",
    "for min_df in [0.1,0.2]\n",
    "for max_df in [0.8,0.9,1.0]\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "'C': None,\n",
    "'ngram_range': None,\n",
    "'min_df': None,\n",
    "'max_df': None,\n",
    "}\n",
    "\n",
    "best_cv_score_22 = 0\n",
    "best_cv_score_std_22 = None\n",
    "best_model_22 = None\n",
    "best_parameters_22 = None\n",
    "param = 0\n",
    "for C, ngram_range, min_df, max_df in gridsearch_params:\n",
    "    cv_scores = []\n",
    "    param +=1\n",
    "    print('param', param)\n",
    "    fold = 0\n",
    "\n",
    "    params['C'] = C\n",
    "    params['ngram_range'] = ngram_range\n",
    "    params['min_df'] = min_df\n",
    "    params['max_df'] = max_df\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        fold +=1\n",
    "        print('fold', fold)\n",
    "\n",
    "        tfidf = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "        temp = tfidf.fit_transform(X_train['mod_text'].iloc[train_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        feature_names = tfidf.get_feature_names()\n",
    "        X_1_tr = pd.DataFrame(dense, columns=feature_names)\n",
    "        temp = tfidf.transform(X_train['mod_text'].iloc[test_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        X_1_te = pd.DataFrame(dense, columns=feature_names)\n",
    "        \n",
    "        X_3_tr = X_train[['neg', 'neu', 'pos', 'compound']].iloc[train_index]\n",
    "        X_3_tr = X_3_tr.reset_index(drop=True)\n",
    "        X_3_te = X_train[['neg', 'neu', 'pos', 'compound']].iloc[test_index]\n",
    "        X_3_te = X_3_te.reset_index(drop=True)\n",
    "        \n",
    "        X_cv_train = pd.concat([X_1_tr, X_3_tr], axis=1)\n",
    "        X_cv_test = pd.concat([X_1_te, X_3_te], axis=1)\n",
    "        y_cv_train = df['Popular'].iloc[train_index]\n",
    "        y_cv_test = df['Popular'].iloc[test_index]\n",
    "\n",
    "        log = LogisticRegression(C=C, solver='lbfgs', n_jobs=-1)\n",
    "        log.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "        cv_scores.append(log.score(X_cv_test, y_cv_test))\n",
    "\n",
    "    if np.mean(cv_scores) > best_cv_score_22:\n",
    "        best_cv_score_22 = np.mean(cv_scores)\n",
    "        best_cv_score_std_22 = np.std(cv_scores)\n",
    "        best_parameters_22 = params\n",
    "        best_model_22 = [tfidf, log]\n",
    "\n",
    "    print('score', np.mean(cv_scores))\n",
    "    print('params', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "(C, ngram_range, min_df, max_df, n_components)\n",
    "for C in np.logspace(-3,5,1000)\n",
    "for ngram_range in [(1,1),(1,2),(1,3)]\n",
    "for min_df in [0.1,0.2]\n",
    "for max_df in [0.8,0.9,1.0]\n",
    "for n_components in range(5,30)\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "'C': None,\n",
    "'ngram_range': None,\n",
    "'min_df': None,\n",
    "'max_df': None,\n",
    "'n_components': None\n",
    "}\n",
    "\n",
    "best_cv_score_23 = 0\n",
    "best_cv_score_std_23 = None\n",
    "best_model_23 = None\n",
    "best_parameters_23 = None\n",
    "param = 0\n",
    "for C, ngram_range, min_df, max_df, n_components in gridsearch_params:\n",
    "    cv_scores = []\n",
    "    param +=1\n",
    "    print('param', param)\n",
    "    fold = 0\n",
    "\n",
    "    params['C'] = C\n",
    "    params['ngram_range'] = ngram_range\n",
    "    params['min_df'] = min_df\n",
    "    params['max_df'] = max_df\n",
    "    params['n_components'] = n_components\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        fold +=1\n",
    "        print('fold', fold)\n",
    "\n",
    "        tfidf = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "        temp = tfidf.fit_transform(X_train['mod_text'].iloc[train_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        feature_names = tfidf.get_feature_names()\n",
    "        X_1_tr = pd.DataFrame(dense, columns=feature_names)\n",
    "        temp = tfidf.transform(X_train['mod_text'].iloc[test_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        X_1_te = pd.DataFrame(dense, columns=feature_names)\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=n_components, n_jobs=-1)\n",
    "        temp = lda.fit_transform(X_1_tr)\n",
    "        feature_names = ['topic {}'.format(i) for i in range(1, n_components +1)]\n",
    "        X_2_tr = pd.DataFrame(temp, columns=feature_names)\n",
    "        temp = lda.transform(X_1_te)\n",
    "        X_2_te = pd.DataFrame(temp, columns=feature_names)\n",
    "\n",
    "        \n",
    "        X_3_tr = X_train[['neg', 'neu', 'pos', 'compound']].iloc[train_index]\n",
    "        X_3_tr = X_3_tr.reset_index(drop=True)\n",
    "        X_3_te = X_train[['neg', 'neu', 'pos', 'compound']].iloc[test_index]\n",
    "        X_3_te = X_3_te.reset_index(drop=True)\n",
    "        \n",
    "        X_cv_train = pd.concat([X_2_tr, X_3_tr], axis=1)\n",
    "        X_cv_test = pd.concat([X_2_te, X_3_te], axis=1)\n",
    "        y_cv_train = df['Popular'].iloc[train_index]\n",
    "        y_cv_test = df['Popular'].iloc[test_index]\n",
    "\n",
    "        log = LogisticRegression(C=C, solver='lbfgs', n_jobs=-1)\n",
    "        log.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "        cv_scores.append(log.score(X_cv_test, y_cv_test))\n",
    "\n",
    "    if np.mean(cv_scores) > best_cv_score_23:\n",
    "        best_cv_score_23 = np.mean(cv_scores)\n",
    "        best_cv_score_std_23 = np.std(cv_scores)\n",
    "        best_parameters_23 = params\n",
    "        best_model_23 = [tfidf, lda, log]\n",
    "\n",
    "    print('score', np.mean(cv_scores))\n",
    "    print('params', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "(C, ngram_range, min_df, max_df, n_components)\n",
    "for C in np.logspace(-3,5,1000)\n",
    "for ngram_range in [(1,1),(1,2),(1,3)]\n",
    "for min_df in [0.1,0.2]\n",
    "for max_df in [0.8,0.9,1.0]\n",
    "for n_components in range(5,30)\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "'C': None,\n",
    "'ngram_range': None,\n",
    "'min_df': None,\n",
    "'max_df': None,\n",
    "'n_components': None\n",
    "}\n",
    "\n",
    "best_cv_score_31 = 0\n",
    "best_cv_score_std_31 = None\n",
    "best_model_31 = None\n",
    "best_parameters_31 = None\n",
    "param = 0\n",
    "for C, ngram_range, min_df, max_df, n_components in gridsearch_params:\n",
    "    cv_scores = []\n",
    "    param +=1\n",
    "    print('param', param)\n",
    "    fold = 0\n",
    "\n",
    "    params['C'] = C\n",
    "    params['ngram_range'] = ngram_range\n",
    "    params['min_df'] = min_df\n",
    "    params['max_df'] = max_df\n",
    "    params['n_components'] = n_components\n",
    "    \n",
    "    for train_index, test_index in kf.split(df):\n",
    "        fold +=1\n",
    "        print('fold', fold)\n",
    "\n",
    "        tfidf = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "        temp = tfidf.fit_transform(df['mod_text'].iloc[train_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        feature_names = tfidf.get_feature_names()\n",
    "        X_1_tr = pd.DataFrame(dense, columns=feature_names)\n",
    "        temp = tfidf.transform(df['mod_text'].iloc[test_index])\n",
    "        dense = temp.todense().tolist()\n",
    "        X_1_te = pd.DataFrame(dense, columns=feature_names)\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=n_components, n_jobs=-1)\n",
    "        temp = lda.fit_transform(X_1_tr)\n",
    "        feature_names = ['topic {}'.format(i) for i in range(1, n_components +1)]\n",
    "        X_2_tr = pd.DataFrame(temp, columns=feature_names)\n",
    "        temp = lda.transform(X_1_te)\n",
    "        X_2_te = pd.DataFrame(temp, columns=feature_names)\n",
    "\n",
    "        \n",
    "        X_3_tr = df[['neg', 'neu', 'pos', 'compound']].iloc[train_index]\n",
    "        X_3_tr = X_3_tr.reset_index(drop=True)\n",
    "        X_3_te = df[['neg', 'neu', 'pos', 'compound']].iloc[test_index]\n",
    "        X_3_te = X_3_te.reset_index(drop=True)\n",
    "        \n",
    "        X_cv_train = pd.concat([X_1_tr, X_2_tr, X_3_tr], axis=1)\n",
    "        X_cv_test = pd.concat([X_1_te, X_2_te, X_3_te], axis=1)\n",
    "        y_cv_train = df['Popular'].iloc[train_index]\n",
    "        y_cv_test = df['Popular'].iloc[test_index]\n",
    "\n",
    "        log = LogisticRegression(C=C, solver='lbfgs', n_jobs=-1)\n",
    "        log.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "        cv_scores.append(log.score(X_cv_test, y_cv_test))\n",
    "\n",
    "    if np.mean(cv_scores) > best_cv_score_31:\n",
    "        best_cv_score_31 = np.mean(cv_scores)\n",
    "        best_cv_score_std_31 = np.std(cv_scores)\n",
    "        best_parameters_31 = params\n",
    "        best_model_31 = [tfidf, lda, log]\n",
    "\n",
    "    print('score', np.mean(cv_scores))\n",
    "    print('params', params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
